# æ”¯æŒå›¾ç‰‡ä¸‹è½½çš„å¼€æºçˆ¬è™«æ¨è

## ä¸€ã€æœ€ä½³æ¨èçˆ¬è™«

### 1. Scrapy (Python) - åŠŸèƒ½æœ€å¼ºå¤§
- **GitHub**: https://github.com/scrapy/scrapy
- **ç‰¹ç‚¹**: 
  - âœ… å®Œç¾æ”¯æŒæ–‡æœ¬æ•°æ®å’Œå›¾ç‰‡åŒæ—¶çˆ¬å–
  - âœ… å†…ç½® `ImagesPipeline` ä¸“é—¨å¤„ç†å›¾ç‰‡ä¸‹è½½
  - âœ… æ”¯æŒè‡ªåŠ¨é‡å‘½åã€å»é‡ã€ç¼©ç•¥å›¾
  - âœ… æ”¯æŒè‡ªå®šä¹‰å›¾ç‰‡å­˜å‚¨è·¯å¾„
  - âœ… é«˜æ€§èƒ½ï¼Œé€‚åˆå¤§è§„æ¨¡çˆ¬å–
  - âœ… ä¸°å¯Œçš„æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒ

### 2. EasySpider (Python) - æœ€æ˜“ç”¨
- **GitHub**: https://github.com/NaiboWang/EasySpider
- **ç‰¹ç‚¹**: 
  - âœ… å¯è§†åŒ–æ“ä½œï¼Œæ— éœ€ç¼–ç¨‹
  - âœ… æ”¯æŒåŒæ—¶æå–æ–‡æœ¬å’Œå›¾ç‰‡
  - âœ… ä¸­æ–‡ç•Œé¢ï¼Œæ“ä½œå‹å¥½
  - âœ… é€‚åˆå¿«é€Ÿå¼€å‘ç®€å•çˆ¬è™«
  - âœ… æ”¯æŒè‡ªå®šä¹‰å­˜å‚¨

## äºŒã€Scrapy è¯¦ç»†ä½¿ç”¨æŒ‡å—

### 1. å®‰è£…
```bash
pip install scrapy pillow
```

### 2. åˆ›å»ºçˆ¬è™«é¡¹ç›®
```bash
scrapy startproject historical_crawler
cd historical_crawler
```

### 3. é…ç½®å›¾ç‰‡ä¸‹è½½
ä¿®æ”¹ `settings.py`:
```python
# å¯ç”¨å›¾ç‰‡ç®¡é“
ITEM_PIPELINES = {
    'scrapy.pipelines.images.ImagesPipeline': 1,
}

# å›¾ç‰‡å­˜å‚¨è·¯å¾„
IMAGES_STORE = './images'

# å›¾ç‰‡ä¸‹è½½è¶…æ—¶
IMAGES_STORE_TIMEOUT = 15

# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
IMAGES_EXPIRES = 90  # 90å¤©è¿‡æœŸ
```

### 4. å®šä¹‰æ•°æ®æ¨¡å‹
åˆ›å»º `items.py`:
```python
import scrapy

class HistoricalPersonItem(scrapy.Item):
    name = scrapy.Field()
    birth_year = scrapy.Field()
    death_year = scrapy.Field()
    dynasty = scrapy.Field()
    description = scrapy.Field()
    image_urls = scrapy.Field()  # å›¾ç‰‡URLåˆ—è¡¨
    images = scrapy.Field()      # ä¸‹è½½åçš„å›¾ç‰‡ä¿¡æ¯

class HistoricalEventItem(scrapy.Item):
    name = scrapy.Field()
    start_year = scrapy.Field()
    end_year = scrapy.Field()
    dynasty = scrapy.Field()
    description = scrapy.Field()
    image_urls = scrapy.Field()
    images = scrapy.Field()
```

### 5. åˆ›å»ºäººç‰©çˆ¬è™«
åˆ›å»º `spiders/person_spider.py`:
```python
import scrapy
from historical_crawler.items import HistoricalPersonItem

class PersonSpider(scrapy.Spider):
    name = "person"
    allowed_domains = ["baike.baidu.com"]
    start_urls = ["https://baike.baidu.com/item/å­”å­"]

    def parse(self, response):
        item = HistoricalPersonItem()
        
        # æå–æ–‡æœ¬ä¿¡æ¯
        item['name'] = response.css('.lemmaWgt-lemmaTitle-title h1::text').get()
        item['description'] = response.css('.lemma-summary').xpath('string(.)').get().strip()
        
        # æå–åŸºæœ¬ä¿¡æ¯
        basic_info = {}
        for info_item in response.css('.basicInfo_M3XoO .itemName_hpSfh'):
            key = info_item.xpath('string(.)').get().strip()
            value = info_item.xpath('following-sibling::div[1]').xpath('string(.)').get().strip()
            basic_info[key] = value
        
        # æå–å›¾ç‰‡URL
        item['image_urls'] = []
        # äººç‰©å¤´åƒ
        avatar = response.css('.summary-pic img::attr(src)').get()
        if avatar:
            item['image_urls'].append(avatar)
        # å†…å®¹ä¸­çš„å›¾ç‰‡
        for img in response.css('.main-content img::attr(src)').getall():
            if img.startswith('http'):
                item['image_urls'].append(img)
        
        yield item
```

### 6. è¿è¡Œçˆ¬è™«
```bash
scrapy crawl person
```

### 7. å›¾ç‰‡å­˜å‚¨ç»“æ„
```
images/
â”œâ”€â”€ full/          # å®Œæ•´å›¾ç‰‡
â”‚   â”œâ”€â”€ abc123.jpg  # è‡ªåŠ¨é‡å‘½åçš„å›¾ç‰‡
â”‚   â””â”€â”€ def456.jpg
â””â”€â”€ thumbs/        # ç¼©ç•¥å›¾(å¯é€‰)
    â””â”€â”€ small/
```

## ä¸‰ã€EasySpider ä½¿ç”¨æŒ‡å—

### 1. å®‰è£…
```bash
# ä¸‹è½½æœ€æ–°ç‰ˆæœ¬
git clone https://github.com/NaiboWang/EasySpider.git
cd EasySpider

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨æœåŠ¡
python -m flask run
```

### 2. å¯è§†åŒ–æ“ä½œ
1. æ‰“å¼€æµè§ˆå™¨è®¿é—® `http://localhost:5000`
2. ç‚¹å‡»"æ–°å»ºçˆ¬è™«"
3. è®¾ç½®çˆ¬å–åœ°å€ï¼ˆå¦‚ç™¾åº¦ç™¾ç§‘äººç‰©é¡µé¢ï¼‰
4. ä½¿ç”¨å¯è§†åŒ–å·¥å…·é€‰æ‹©éœ€è¦æå–çš„å­—æ®µï¼š
   - äººç‰©åç§°
   - å‡ºç”Ÿå¹´ä»½
   - æœä»£
   - ç®€ä»‹
   - å›¾ç‰‡URLï¼ˆé€‰æ‹©å›¾ç‰‡å…ƒç´ çš„srcå±æ€§ï¼‰
5. åœ¨"ä¸‹è½½è®¾ç½®"ä¸­å¯ç”¨å›¾ç‰‡ä¸‹è½½
6. è®¾ç½®å›¾ç‰‡å­˜å‚¨è·¯å¾„
7. å¯åŠ¨çˆ¬å–

## å››ã€Crawlee.js (Node.js) å›¾ç‰‡ä¸‹è½½æ–¹æ¡ˆ

å¦‚æœåšæŒä½¿ç”¨Node.jsï¼ŒCrawlee.jsä¹Ÿæ”¯æŒå›¾ç‰‡ä¸‹è½½ï¼š

```javascript
import { PlaywrightCrawler } from 'crawlee';
import fs from 'fs';
import path from 'path';
import https from 'https';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // æå–äººç‰©ä¿¡æ¯
        const name = await page.textContent('.lemmaWgt-lemmaTitle-title h1');
        const description = await page.textContent('.lemma-summary');
        
        // æå–å›¾ç‰‡
        const imgUrls = await page.$$eval('img', imgs => 
            imgs.map(img => img.src).filter(src => src.startsWith('http'))
        );
        
        // åˆ›å»ºå­˜å‚¨ç›®å½•
        const dirPath = `./images/${name}`;
        if (!fs.existsSync(dirPath)) {
            fs.mkdirSync(dirPath, { recursive: true });
        }
        
        // ä¸‹è½½å›¾ç‰‡
        for (let i = 0; i < imgUrls.length; i++) {
            const imgUrl = imgUrls[i];
            const ext = path.extname(new URL(imgUrl).pathname) || '.jpg';
            const imgPath = path.join(dirPath, `${i}${ext}`);
            
            await downloadImage(imgUrl, imgPath);
            console.log(`ä¸‹è½½å›¾ç‰‡: ${imgPath}`);
        }
        
        // ä¿å­˜äººç‰©ä¿¡æ¯
        const data = { name, description, imgUrls };
        fs.writeFileSync(`${dirPath}/info.json`, JSON.stringify(data, null, 2));
    }
});

// å›¾ç‰‡ä¸‹è½½å‡½æ•°
function downloadImage(url, dest) {
    return new Promise((resolve, reject) => {
        const file = fs.createWriteStream(dest);
        https.get(url, response => {
            response.pipe(file);
            file.on('finish', () => {
                file.close(resolve);
            });
        }).on('error', err => {
            fs.unlink(dest);
            reject(err);
        });
    });
}

// å¯åŠ¨çˆ¬å–
await crawler.run(['https://baike.baidu.com/item/å­”å­']);
```

## äº”ã€é€‰æ‹©å»ºè®®

### ğŸ¯ æ¨èé€‰æ‹© Scrapy
- **ä¼˜åŠ¿**: åŠŸèƒ½æœ€å…¨é¢ï¼Œå›¾ç‰‡å¤„ç†èƒ½åŠ›æœ€å¼º
- **é€‚åˆ**: éœ€è¦é«˜è´¨é‡ã€å¤§è§„æ¨¡çˆ¬å–çš„åœºæ™¯
- **æ³¨æ„**: éœ€è¦PythonåŸºç¡€

### ğŸ¯ å¤‡é€‰é€‰æ‹© EasySpider
- **ä¼˜åŠ¿**: æ— éœ€ç¼–ç¨‹ï¼Œå¿«é€Ÿä¸Šæ‰‹
- **é€‚åˆ**: ç®€å•çˆ¬å–ä»»åŠ¡ï¼Œæˆ–éæŠ€æœ¯äººå‘˜ä½¿ç”¨
- **æ³¨æ„**: å¤æ‚éœ€æ±‚å¯èƒ½å—é™åˆ¶

## å…­ã€é›†æˆåˆ°ç°æœ‰é¡¹ç›®

### Scrapy ä¸ Node.js é¡¹ç›®é›†æˆ
```bash
# 1. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
venv/bin/activate  # Linux/Mac
venv\Scripts\activate  # Windows

# 2. å®‰è£…ä¾èµ–
pip install scrapy pillow

# 3. è¿è¡Œçˆ¬è™«
scrapy crawl person

# 4. Node.js è¯»å–çˆ¬å–ç»“æœ
# åœ¨Node.jsä»£ç ä¸­:
const fs = require('fs');
const persons = JSON.parse(fs.readFileSync('./data/persons.json', 'utf8'));
```

## ä¸ƒã€å®é™…åº”ç”¨ç¤ºä¾‹

### çˆ¬å–å­”å­ä¿¡æ¯å’Œå›¾ç‰‡çš„å®Œæ•´æµç¨‹
1. **å®‰è£…Scrapy** â†’ 2. **åˆ›å»ºçˆ¬è™«é¡¹ç›®** â†’ 3. **é…ç½®å›¾ç‰‡ä¸‹è½½** â†’ 4. **ç¼–å†™çˆ¬è™«ä»£ç ** â†’ 5. **è¿è¡Œçˆ¬è™«**

**ç»“æœè¾“å‡º**:
```
# æ–‡æœ¬æ•°æ® (items.json)
{
  "name": "å­”å­",
  "birth_year": "-551",
  "dynasty": "æ˜¥ç§‹æœ«æœŸ",
  "description": "å­”å­ï¼ˆå…¬å…ƒå‰551å¹´9æœˆ28æ—¥â€”å‰479å¹´4æœˆ11æ—¥ï¼‰...",
  "image_urls": ["https://example.com/confucius.jpg"]
}

# å›¾ç‰‡æ–‡ä»¶ (images/full/abc123.jpg)
# è‡ªåŠ¨ä¸‹è½½å¹¶å­˜å‚¨çš„å­”å­å›¾ç‰‡
```

## å…«ã€æ€»ç»“

| çˆ¬è™« | è¯­è¨€ | å›¾ç‰‡æ”¯æŒ | æ˜“ç”¨æ€§ | æ€§èƒ½ | æ¨èæŒ‡æ•° |
|------|------|----------|--------|------|----------|
| Scrapy | Python | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| EasySpider | Python | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| Crawlee.js | JS/TS | â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |

**æœ€ç»ˆæ¨è**: **Scrapy** æ˜¯æœ€é€‚åˆæ‚¨éœ€æ±‚çš„å¼€æºçˆ¬è™«ï¼Œå®ƒå®Œç¾æ”¯æŒåŒæ—¶çˆ¬å–äººç‰©/äº‹ä»¶ä¿¡æ¯å’Œå¯¹åº”å›¾ç‰‡ï¼Œæä¾›ä¸“ä¸šçš„å›¾ç‰‡å¤„ç†èƒ½åŠ›ï¼Œé€‚åˆé•¿æœŸç¨³å®šä½¿ç”¨ã€‚